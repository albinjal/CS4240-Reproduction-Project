{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupNorm(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super(GroupNorm,self).__init__()\n",
    "        self.gn = nn.GroupNorm(num_groups=32,num_channels=channels,eps=1e-6,affine=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.gn(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.Residual_block = nn.Sequential(\n",
    "            GroupNorm(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels,out_channels,3,1,1),\n",
    "            GroupNorm(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels,out_channels,3,1,1),\n",
    "        )\n",
    "        if in_channels != out_channels:\n",
    "            self.channel_up =  nn.Conv2d(in_channels,out_channels,1,1,0) # 1x1 kernel is applied\n",
    "            # so no padding is needed since 1x1 kernels keep the spatial dimensions the same \n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.in_channels != self.out_channels:\n",
    "            upchannel = self.channel_up(x)\n",
    "            res_block = self.Residual_block(x)\n",
    "            return self.channel_up(x) + self.Residual_block(x) \n",
    "        \n",
    "        else:\n",
    "            return x + self.Residual_block(x)\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super(UpSampleBlock,self).__init__()\n",
    "        self.conv = nn.Conv2d(channels,channels,3,1,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.interpolate(x, scale_factor=2.0) # double height and width\n",
    "        # fill in spot by nearest neighbour\n",
    "        return self.conv(x)\n",
    "    \n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super(DownSampleBlock,self).__init__()\n",
    "        self.conv = nn.Conv2d(channels,channels,3,2,0)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        pad = (0,1,0,1)\n",
    "        x = F.pad(x,pad,mode=\"constant\",value=0) # pad 0 on the height \n",
    "        # dimension and on the width dimension (done so that the dimension\n",
    "        # will be half rounded to the nearest integer)\n",
    "        return self.conv(x) \n",
    "\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        self.in_channels = channels\n",
    "\n",
    "        self.gn = GroupNorm(channels) \n",
    "\n",
    "        self.q = nn.Conv2d(channels,channels,1,1,0)\n",
    "        self.k = nn.Conv2d(channels,channels,1,1,0)\n",
    "        self.v = nn.Conv2d(channels,channels,1,1,0)\n",
    "\n",
    "        self.project_out = nn.Conv2d(channels,channels,1,1,0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embedding = self.gn(x)\n",
    "        q = self.q(embedding)\n",
    "        k = self.k(embedding)\n",
    "        v = self.v(embedding)\n",
    "\n",
    "        b,c,h,w = q.shape\n",
    "\n",
    "        q = q.reshape(b,c,h*w) # put the image into a \n",
    "        # single vector instead of a 2d matrix\n",
    "        q = q.permute(0,2,1) # effectively transposing Q\n",
    "        k = k.reshape(b,c,h*w)\n",
    "        v = v.reshape(b,c,h*w)\n",
    "\n",
    "        attention = torch.bmm(q,k) # performs matrix-matrix multiplication\n",
    "        # for all the batches so we obtain Q^T*K\n",
    "        attention *= (int(c)**(-.5))\n",
    "        attention = F.softmax(attention,dim=2)\n",
    "        attention = attention.permute(0,2,1) # effectively transposing again\n",
    "        # probably not really necessary\n",
    "\n",
    "        A = torch.bmm(v,attention)  \n",
    "\n",
    "        A = A.reshape(b,c,h,w)\n",
    "\n",
    "        return x+A\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#from torchsummary import summary\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(Encoder,self).__init__()\n",
    "        channels = [128,128,128,256,256,512] \n",
    "        attention_resolutions = [16]\n",
    "        number_res_blocks = 2\n",
    "        resolution = 256\n",
    "        layers = [nn.Conv2d(args.image_channels,channels[0],3,1,1)]\n",
    "        for i in range(len(channels)-1):\n",
    "            in_channels = channels[i]\n",
    "            out_channels = channels[i+1]\n",
    "            for j in range(number_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels,out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attention_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            if i!=len(channels)-2:\n",
    "                layers.append(DownSampleBlock(channels[i+1]))\n",
    "                resolution //=2\n",
    "        layers.append(ResidualBlock(channels[-1],channels[-1]))\n",
    "        layers.append(NonLocalBlock(channels[-1]))\n",
    "        layers.append(ResidualBlock(channels[-1],channels[-1]))\n",
    "        layers.append(GroupNorm(channels[-1]))\n",
    "        layers.append(nn.SiLU())\n",
    "        layers.append(nn.Conv2d(channels[-1],args.latent_dim,3,1,1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        ##summary(self.model,)\n",
    "        #print(\"Total length model\", len(self.model))\n",
    "        #for i,layer in enumerate(self.model):\n",
    "            #print(\"Layer:\",i)\n",
    "            #print(\"Input dims:\", x.shape)\n",
    "            #classname = layer.__class__.__name__\n",
    "            #if classname == \"NonLocalBlock\" and i==14:\n",
    "                #print(layer.in_channels)\n",
    "                ##print(layer.out_channels)\n",
    "                \n",
    "            #print(\"layername\",classname)\n",
    "\n",
    "            #x=layer(x)\n",
    "            #print(\"Output dims: \", x.shape)\n",
    "        #return x\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(Decoder,self).__init__()\n",
    "        channels = [512,256,256,128,128]\n",
    "        attention_resolution = [16]\n",
    "        number_res_blocks = 3\n",
    "        resolution = 16\n",
    "\n",
    "        in_channels = channels[0]\n",
    "        layers = [\n",
    "            nn.Conv2d(args.latent_dim,in_channels,3,1,1),\n",
    "            ResidualBlock(in_channels,in_channels),\n",
    "            NonLocalBlock(in_channels),\n",
    "            ResidualBlock(in_channels,in_channels)\n",
    "            ]\n",
    "        for i in range(len(channels)):\n",
    "            out_channels=channels[i]\n",
    "            for j in range(number_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels,out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attention_resolution:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            if i!=0:\n",
    "                layers.append(UpSampleBlock(in_channels))\n",
    "                resolution *= 2\n",
    "        layers.append(GroupNorm(in_channels))\n",
    "        layers.append(nn.SiLU())\n",
    "        layers.append(nn.Conv2d(in_channels,args.image_channels,3,1,1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Codebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codebook(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(Codebook,self).__init__()\n",
    "        self.num_codebook_vectors = args.num_codebook_vectors\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.beta = args.beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_codebook_vectors,self.latent_dim) # matrix with as rows the different embedding vectors\n",
    "\n",
    "        # takes as input tensor with indices, output will be a tensor containing all the requested embedding vectors that corr with the indices\n",
    "        self.embedding.weight.data.uniform_(-1.0/self.num_codebook_vectors,1.0/self.num_codebook_vectors) #the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized uniformly now\n",
    "\n",
    "    def forward(self,z):\n",
    "        # z is normally of shape (batch_size,channels,height, width), after permutation its (batch_size, height,width,channels)\n",
    "        z = z.permute(0,2,3,1).contiguous() # prepending latent vectors for finding the minimal distance to the codebook vectors\n",
    "        z_flattened = z.view(-1,self.latent_dim)\n",
    "\n",
    "        d = torch.sum(z_flattened**2,dim=1,keepdim=True)+\\\n",
    "            torch.sum(self.embedding.weight**2, dim=1)-\\\n",
    "            2*(torch.matmul(z_flattened,self.embedding.weight.t()))\n",
    "\n",
    "        min_encoding_indices = torch.argmin(d,dim=1)\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
    "        loss = torch.mean((z_q.detach()-z)**2)+ self.beta * torch.mean((z_q-z.detach())**2)\n",
    "        # above we first remove the gradient from the quantized latent vectors from the gradient flow and substract it from the original latent vector\n",
    "        # in the second part we remove tha gradient from the original latent vector and keep the one of the quantized latent vector and substract them \n",
    "\n",
    "        z_q = z + (z_q-z).detach() # here we make sure that z_q has the gradient of z but keeps its quantized value\n",
    "        z_q = z_q.permute(0,3,1,2) \n",
    "\n",
    "        return z_q, min_encoding_indices, loss \n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VQGAN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(VQGAN, self).__init__()\n",
    "        self.encoder = Encoder(args).to(device=args.device)\n",
    "        self.decoder= Decoder(args).to(device=args.device)\n",
    "        self.codebook = Codebook(args).to(device=args.device)\n",
    "        self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim,1).to(device=args.device)\n",
    "        self.post_quant_conv = nn.Conv2d(args.latent_dim,args.latent_dim,1).to(device=args.device)\n",
    "    \n",
    "    def forward(self,imgs):\n",
    "        enc_imgs = self.encoder(imgs)\n",
    "        pre_quant_conv = self.quant_conv(enc_imgs)\n",
    "        quantized_imgs, quantized_indices,q_loss = self.codebook(pre_quant_conv)\n",
    "        post_quant_conv_mapping = self.post_quant_conv(quantized_imgs)\n",
    "        decoded_imgs = self.decoder(post_quant_conv_mapping)\n",
    "\n",
    "        return decoded_imgs,quantized_indices, q_loss\n",
    "    \n",
    "    def encode(self,imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        quantized_imgs, indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "\n",
    "        return quantized_imgs,indices,q_loss\n",
    "    \n",
    "    def decode(self,z):\n",
    "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
    "        decoded_imgs = self.decoder(post_quant_conv_mapping)\n",
    "        return decoded_imgs\n",
    "\n",
    "    def calculate_lambda(self,perceptual_loss,gan_loss):\n",
    "        last_layer = self.decoder.model[-1]\n",
    "        last_layer_weight = last_layer.weight\n",
    "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss,last_layer_weight,retain_graph=True)[0]# retain graph makes sure the computational \n",
    "        # graph won't get discarded after calling .grad, this way the gan loss can also be calculated wrt to the weights `[0]` here means we get the \n",
    "        # gradient of the zero'th element in the sequence of inputs we've given\n",
    "        gan_loss_grads = torch.autograd.grad(gan_loss,last_layer_weight,retain_graph=True)[0]\n",
    "        \n",
    "        lamb = torch.norm(perceptual_loss_grads)/(torch.norm(gan_loss_grads)+1e-4)\n",
    "        lamb = torch.clamp(lamb,0,1e4).detach()\n",
    "        return .8* lamb\n",
    "\n",
    "    @staticmethod\n",
    "    def adopt_weight(disc_factor,i,threshold,value=0.):\n",
    "        if i<threshold:\n",
    "            disc_factor = value\n",
    "        return disc_factor\n",
    "\n",
    "    def load_checkpoint(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator (copy of cycleGAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,args,num_filters_last=64,n_layers=3):\n",
    "        super(Discriminator,self).__init__()\n",
    "\n",
    "        # layers start with a convolutional layer that takes an input image with args.image_channels number of channels\n",
    "        layers = [nn.Conv2d(args.image_channels,num_filters_last,4,2,1), nn.LeakyReLU(.2)]\n",
    "        num_filters_mult=1\n",
    "        # we multiply the number of outputted filters by 2 every iteration\n",
    "        # untill we would multiply the original number of filters by 8, after that the number of \n",
    "        # features stay the same\n",
    "        for i in range(1,n_layers+1):\n",
    "            num_filters_mult_last = num_filters_mult\n",
    "            num_filters_mult = min(2**i,8)\n",
    "\n",
    "            layers += [\n",
    "                nn.Conv2d(num_filters_last*num_filters_mult_last, num_filters_last* num_filters_mult,4, 2 if i <n_layers else 1,bias=False),\n",
    "                nn.BatchNorm2d(num_filters_last*num_filters_mult),\n",
    "                nn.LeakyReLU(.2,True)\n",
    "            ]\n",
    "        layers.append(nn.Conv2d(num_filters_last*num_filters_mult,1,4,1,1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        #print(\"Total length model\", len(self.model))\n",
    "        #for i,layer in enumerate(self.model):\n",
    "            #print(\"Layer:\",i)\n",
    "            #print(\"Input dims:\", x.shape)\n",
    "            #classname = layer.__class__.__name__\n",
    "            #if classname == \"Conv2d\" and i==6:\n",
    "                #print(layer.in_channels)\n",
    "                #print(layer.out_channels)\n",
    "                \n",
    "            #print(\"layername\",classname)\n",
    "\n",
    "            #x=layer(x)\n",
    "            #print(\"Output dims: \", x.shape)\n",
    "        #return x\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "URL_DCT = {\n",
    "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
    "}\n",
    "\n",
    "CKPT_DCT = {\n",
    "    \"vgg_lpips\": \"vgg.pth\"\n",
    "}\n",
    "\n",
    "def download(url, local_path, chunk_size=1024):\n",
    "    # creates the necessary directory for the local path using os.makedirs\n",
    "    os.makedirs(os.path.split(local_path)[0],exist_ok=True)\n",
    "    # sends get request to specified url with stream == True so that the response \n",
    "    # is streamed instead of loaded into memory at once\n",
    "    with requests.get(url,stream=True) as r:\n",
    "        total_size = int(r.headers.get(\"content-length\",0))\n",
    "        # creates progress bar to display the download progress, the total= Total_size\n",
    "        # which is the the size of the file being downloaded retrieved from content\n",
    "        # length header \n",
    "        with tqdm(total=total_size,unit=\"B\",unit_scale=True) as pbar:\n",
    "            with open(local_path,\"wb\") as f:\n",
    "                for data in r.iter_content(chunk_size=chunk_size):\n",
    "                    if data:\n",
    "                        # we iterate over the data stream and write to the\n",
    "                        # file\n",
    "                        f.write(data)\n",
    "                        # progress bar is updated\n",
    "                        pbar.update(chunk_size)\n",
    "\n",
    "def get_ckpt_path(name,root):\n",
    "    assert name in URL_DCT\n",
    "    path = os.path.join(root,CKPT_DCT[name])\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {name} model from {URL_DCT[name]} to {path}\")\n",
    "        download(URL_DCT[name], path)\n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels=1):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(in_channels,out_channels,1,1,0,bias=False)\n",
    "        )\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16,self).__init__()\n",
    "        vgg_pretrained_features = vgg16(pretrained=True).features\n",
    "        slices = [vgg_pretrained_features[i] for i in range(30)]\n",
    "        self.slice1 = nn.Sequential(*slices[0:4])\n",
    "        self.slice2 = nn.Sequential(*slices[4:9])\n",
    "        self.slice3 = nn.Sequential(*slices[9:16])\n",
    "        self.slice4 = nn.Sequential(*slices[16:23])\n",
    "        self.slice5 = nn.Sequential(*slices[23:30])\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        h=x\n",
    "        #print(\"Total length model slice1\", len(self.slice1))\n",
    "        #for i,layer in enumerate(self.slice1):\n",
    "            #print(\"Layer:\",i)\n",
    "            #print(\"Input dims:\", h.shape)\n",
    "            #classname = layer.__class__.__name__\n",
    "            #if classname == \"NonLocalBlock\" and i==14:\n",
    "                #print(layer.in_channels)\n",
    "                ##print(layer.out_channels)\n",
    "                \n",
    "            #print(\"layername\",classname)\n",
    "\n",
    "            #h=layer(h)\n",
    "            #print(\"Output dims: \", h.shape)\n",
    "        h = self.slice1(x)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2','relu2_2','relu3_3', \"relu4_3\",\"relu5_3\"])\n",
    "        return vgg_outputs(h_relu1,h_relu2,h_relu3,h_relu4,h_relu5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_tensor(x):\n",
    "    '''\n",
    "    Computes the L2 norm of each channel vector of of input tensor x,\n",
    "    and returns the normalized version of x\n",
    "    '''\n",
    "    norm_factor = torch.sqrt(torch.sum(x**2,dim=1,keepdim=True))\n",
    "    return x/(norm_factor+1e-10)\n",
    "\n",
    "def spatial_average(x):\n",
    "    '''\n",
    "    Computes the average of each channel over the spatial dimensions of the tensor\n",
    "    When x has shape (batch_size,num_channels,height,width) then the output tensor\n",
    "    will have shape (batch_size,num_channels, 1,1), the scalar represents how similar \n",
    "    the real and fake images are.\n",
    "    '''\n",
    "    return x.mean([2,3],keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPIPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LPIPS,self).__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        self.channels = [64,128,256,512,512]\n",
    "        self.vgg = VGG16()\n",
    "\n",
    "        self.lins = nn.ModuleList([\n",
    "            NetLinLayer(self.channels[0]),\n",
    "            NetLinLayer(self.channels[1]),\n",
    "            NetLinLayer(self.channels[2]),\n",
    "            NetLinLayer(self.channels[3]),\n",
    "            NetLinLayer(self.channels[4]),\n",
    "        ])\n",
    "\n",
    "        self.load_from_pretrained()\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def load_from_pretrained(self,name=\"vgg_lpips\"):\n",
    "        ckpt = get_ckpt_path(name,\"vgg_lpips\")\n",
    "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")),strict=False)\n",
    "\n",
    "    def forward(self,real_img,fake_img):\n",
    "        features_real = self.vgg(self.scaling_layer(real_img))\n",
    "        features_fake = self.vgg(self.scaling_layer(fake_img))\n",
    "\n",
    "        diffs = {}\n",
    "\n",
    "        for i in range(len(self.channels)):\n",
    "            diffs[i] = (norm_tensor(features_real[i])-norm_tensor(features_fake[i]))**2\n",
    "        \n",
    "        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n",
    "\n",
    "# The reason for the scaling layer class is to preprocess input images to match the \n",
    "# expected input format of the VGG-16 network used to compute image feature representations\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer,self).__init__()\n",
    "        # creates tensor with torch.tensor([-.030,-.088,-1.88]) but adds new dimension\n",
    "        # of size 1 at the start, : selects all elements along the original tensor resulting\n",
    "        # in shape (1,3), then two additional dimensions are added using None indexing\n",
    "        # resulting in shape (1,3,1), the third None adds a new dimension of size 1 at the\n",
    "        # fourth position (1,3,1,1)\n",
    "        # equivalent: self.register_buffer(\"shift\", torch.tensor([-0.030, -0.088, -1.88]).unsqueeze(0).unsqueeze(2).unsqueeze(3))\n",
    "\n",
    "        self.register_buffer(\"shift\",torch.tensor([-.030,-.088,-1.88])[None,:,None,None])\n",
    "        self.register_buffer(\"scale\",torch.tensor([.458,.448,.45])[None,:,None,None])\n",
    "        # dimensions are added to make it broadcastable since the images will be of size\n",
    "        # (batch_size, 3, height, width)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return (x-self.shift)/self.scale\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data utils\n",
    "\n",
    "class ImagePaths(Dataset):\n",
    "    def __init__(self,path,size=None):\n",
    "        self.size = size\n",
    "\n",
    "        self.images = [os.path.join(path,file) for file in os.listdir(path)]\n",
    "        self._length = len(self.images)\n",
    "\n",
    "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
    "        self.cropper = albumentations.CenterCrop(height=self.size,width=self.size)\n",
    "        self.preprocessor = albumentations.Compose([self.rescaler,self.cropper])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "    \n",
    "    def preprocess_image(self,image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image.convert(\"RGB\")\n",
    "    \n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        # the original image has colors 0-255 pixel values (8bit)\n",
    "        # so dividing by 127.5 will make the values range from [0,2].\n",
    "        # then dividing by -1 will give you a range of [-1,1]\n",
    "        image = (image/127.5-1.0).astype(np.float32)\n",
    "        # normal PIL library will make channels the last dimension\n",
    "        # so we transpose to make the channel go first \n",
    "        image = image.transpose(2,0,1)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.preprocess_image(self.images[index])\n",
    "        return img \n",
    "\n",
    "def load_data(args):\n",
    "    train_data = ImagePaths(args.dataset_path,size=256)\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module utils (encoder, decoder)\n",
    "\n",
    "# for initializing the weights of certain classes properly\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, mean=0.0, std=0.02)\n",
    "    elif classname.find(\"BatchNorm\")!=-1:\n",
    "        nn.init.normal_(m.weight.data,mean=1.0,std=.02)\n",
    "        nn.init.constant_(m.bias.data,0)\n",
    "\n",
    "#plots images of the transformer stage\n",
    "def plot_images(images):\n",
    "    x = images[\"inputs\"]\n",
    "    reconstruction = images[\"reconstructions\"]\n",
    "    sample_half = images[\"samples_half\"]\n",
    "    sample_nopix = images[\"samples_nopix\"]\n",
    "    sample_det = images[\"samples_det\"]\n",
    "    \n",
    "    fig, axarr = plt.subplots(1,5)\n",
    "    axarr[0].imshow(x)\n",
    "    axarr[1].imshow(reconstruction)\n",
    "    axarr[2].imshow(sample_half)\n",
    "    axarr[3].imshow(sample_nopix)\n",
    "    axarr[4].imshow(sample_det)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the VQ-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils as vutils\n",
    "\n",
    "class TrainVQGAN:\n",
    "    def __init__(self,args):\n",
    "        self.vqgan = VQGAN(args).to(device=args.device)\n",
    "        self.discriminator = Discriminator(args).to(device=args.device)\n",
    "        self.discriminator.apply(weights_init)\n",
    "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
    "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
    "\n",
    "        self.prepare_training()\n",
    "\n",
    "        self.train(args)\n",
    "\n",
    "    def configure_optimizers(self,args):\n",
    "        lr = args.learning_rate\n",
    "        opt_vq = torch.optim.Adam(\n",
    "            list(self.vqgan.encoder.parameters()) +\n",
    "            list(self.vqgan.decoder.parameters()) +\n",
    "            list(self.vqgan.codebook.parameters()) +\n",
    "            list(self.vqgan.quant_conv.parameters()) +\n",
    "            list(self.vqgan.post_quant_conv.parameters()) \n",
    "        )\n",
    "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
    "                                    lr=lr,eps=1e-8,betas = (args.beta1,args.beta2))\n",
    "        return opt_vq,opt_disc\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_training():\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    def train(self,args):\n",
    "        train_dataset = load_data(args)\n",
    "        steps_per_epoch = len(train_dataset)\n",
    "        for epoch in range(args.epochs):\n",
    "            with tqdm(range(len(train_dataset))) as pbar:\n",
    "                for i,imgs in zip(pbar,train_dataset):\n",
    "                    imgs = imgs.to(device=args.device)\n",
    "                    decoded_imgs, _ ,q_loss = self.vqgan(imgs)\n",
    "\n",
    "                    disc_real = self.discriminator(imgs)\n",
    "                    disc_fake = self.discriminator(decoded_imgs)\n",
    "\n",
    "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i,threshold=args.disc_start)\n",
    "\n",
    "                    perceptual_loss = self.perceptual_loss(imgs,decoded_imgs)\n",
    "                    rec_loss = torch.abs(imgs-decoded_imgs)\n",
    "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss+ args.rec_loss_factor*rec_loss\n",
    "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
    "                    g_loss = -torch.mean(disc_fake)\n",
    "\n",
    "\n",
    "                    #$\\lambda = \\frac{\\nabla_{G_L}[\\mathcal{L}_{rec}]}{\\nabla_{G_L}[\\mathcal{L}_{GAN}]+\\delta}$\n",
    "                    lam = self.vqgan.calculate_lambda(perceptual_rec_loss,g_loss)\n",
    "                    vq_loss = perceptual_rec_loss * q_loss+ disc_factor * lam * g_loss\n",
    "\n",
    "                    d_loss_real = torch.mean(F.relu(1.0-disc_real)) #hinge loss\n",
    "                    d_loss_fake = torch.mean(F.relu(1.0+disc_fake)) #hinge loss, inspired by svm's\n",
    "\n",
    "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
    "\n",
    "                    self.opt_vq.zero_grad()\n",
    "                    vq_loss.backward(retain_graph=True)\n",
    "\n",
    "                    self.opt_disc.zero_grad()\n",
    "                    gan_loss.backward()\n",
    "\n",
    "                    self.opt_vq.step()\n",
    "                    self.opt_disc.step()\n",
    "                    \n",
    "                    if i %10 ==0:\n",
    "                        with torch.no_grad():\n",
    "                            real_fake_images = torch.cat((imgs[:4],decoded_imgs.add(1).mul(.5)[:4]))\n",
    "                            vutils.save_image(real_fake_images,os.path.join(\"results\", f\"{epoch}_{i}.jpg\"),nrow=4)\n",
    "                    pbar.set_postfix(\n",
    "                        VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(),5),\n",
    "                        GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(),3)\n",
    "                    )\n",
    "                    pbar.update(0)\n",
    "                    torch.save(self.vqgan.state_dict(),os.path.join(\"checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/avilu/OneDrive/Documenten/math/deep_learning/project/VQ-GAN_from_scratch\n",
      "/mnt/c/Users/avilu/OneDrive/Documenten/math/deep_learning/project/atari/atari_v1/screens/revenge/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path = os.path.join(parent_dir, \"atari/atari_v1/screens/revenge/1\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# Parser (from github)\n",
    "parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
    "parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
    "parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
    "parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
    "parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
    "parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
    "parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
    "parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
    "parser.add_argument('--batch-size', type=int, default=2, help='Input batch size for training (default: 6)')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 100)')\n",
    "parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.5)')\n",
    "parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.9)')\n",
    "parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 10000)')\n",
    "parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
    "parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
    "parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "args.dataset_path = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avilu/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/avilu/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      " 14%|█▍        | 290/2018 [09:24<57:24,  1.99s/it, GAN_Loss=0, VQ_Loss=1e-5]    "
     ]
    }
   ],
   "source": [
    "train_vq_gan = TrainVQGAN(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8d4a4abcd93d8e0a48174bd90004934477afbdb8087778e02a03a804c3a7a73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
